# app.py
# Description: This is the main frontend application for the RAG chatbot,
# built using Streamlit. It provides a user interface for asking questions
# and displays the answers generated by the RAG model.

import streamlit as st
from rag_utils import load_vectorstore, create_rag_chain, get_answer
import time

# --- Page Configuration ---
st.set_page_config(
    page_title="RAG Chatbot",
    page_icon="ðŸ¤–",
    layout="wide"
)

# --- State Management ---
# Using session_state to persist data across reruns
if 'rag_chain' not in st.session_state:
    st.session_state.rag_chain = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# --- Main Application Logic ---
def main():
    """
    The main function that runs the Streamlit application.
    """
    st.title("ðŸ“„ Katomaran Assistant")
    st.markdown("""
    Welcome! This chatbot uses Retrieval-Augmented Generation (RAG) to answer questions based on your provided PDF documents.
    
    **How to use:**
    1. Make sure you have run `run_once.py` to process your documents.
    2. Type your question in the input box below and press Enter.
    """)

    # --- Initialization Step ---
    # Load the vector store and create the RAG chain only once
    if st.session_state.rag_chain is None:
        with st.spinner("Initializing the RAG chain... Please wait."):
            vectorstore = load_vectorstore()
            if vectorstore:
                st.session_state.rag_chain = create_rag_chain(vectorstore)
                st.success("Initialization complete! The chatbot is ready.")
            else:
                st.error("Failed to initialize. Please check the `vectorstore` directory and run `run_once.py`.")
                st.stop() # Stop the app if initialization fails

    # --- Chat Interface ---
    st.header("Chat with your Documents")

    # Display chat history
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # User input
    if prompt := st.chat_input("Ask a question about your documents"):
        # Add user message to chat history
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Get assistant response
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            with st.spinner("Thinking..."):
                response = get_answer(st.session_state.rag_chain, prompt)
                answer = response.get('result', "Sorry, I couldn't find an answer.")

            # Simulate stream of response with milliseconds delay
            for chunk in answer.split():
                full_response += chunk + " "
                time.sleep(0.05)
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
            
            # Display source documents
            with st.expander("View Sources"):
                if 'source_documents' in response and response['source_documents']:
                    for doc in response['source_documents']:
                        st.info(f"Source: {doc.metadata.get('source', 'N/A')}, Page: {doc.metadata.get('page', 'N/A')}")
                        st.text(doc.page_content[:250] + "...") # Show a snippet
                else:
                    st.warning("No source documents found for this answer.")

        # Add assistant response to chat history
        st.session_state.chat_history.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
